{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of Document term Matrix\n",
    "In this lab session we will take a few small documents and create term document matrix using both term count and tf-idf approaches. The sample documents are from three distinguished topics like politics, religion and science, two from each topic area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a corpus of 6 documents\n",
    "doc1 = \"\"\"Prime minister Modi will be visiting government officials to discuss policies in the parliament. Fiscal \n",
    "administration is to be given high importance as a partys election promise.\"\"\"\n",
    "\n",
    "doc2 = \"\"\"Last year we went on a holy temple tour. Saw various gods and prayed and worshiped them. We are blessed \n",
    "and our sins are washed away. Belief, trust and full faith in God gives almighty’s protection.\"\"\"\n",
    "\n",
    "doc3 = \"\"\"Mathematics is a science dealing with quantitative methods to solve problems. It deals with differential \n",
    "equations and methodical analysis of linear systems. Vector algebra is also a branch of mathematics. Quantitative \n",
    "analysis helps in algebraically solving non linear as well as linear systems in multi dimensional space.\"\"\"\n",
    "\n",
    "doc4 = \"\"\"Faith and trust play great role in our life. Belief in God, prayer and worship at temples instills confidence. \n",
    "Have trust in holy God, avoid sin for almighty’s blessings.\"\"\"\n",
    "\n",
    "doc5 = \"\"\"Recent elections have shown party promising good governance policy wins parliament and chance for official \n",
    "administration. Ministers have to plan fiscal and social plans quickly.\"\"\"\n",
    "\n",
    "doc6 = \"\"\"Linear algebra is a branch of mathematical sciences. It deals with linear equations and vector spaces and \n",
    "provides strong methods to solve them. It analyzes a system of equations as an n dimensional space and quantifies it \n",
    "to obtain solutions to linear problems.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the documents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "tokenized_doc1=word_tokenize(doc1)\n",
    "tokenized_doc2=word_tokenize(doc2)\n",
    "tokenized_doc3=word_tokenize(doc3)\n",
    "tokenized_doc4=word_tokenize(doc4)\n",
    "tokenized_doc5=word_tokenize(doc5)\n",
    "tokenized_doc6=word_tokenize(doc6)\n",
    "print(\"Tokenized document: \", tokenized_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_doc1 = [];\n",
    "for w in tokenized_doc1:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc1.append(w)\n",
    "filtered_doc2 = [];\n",
    "for w in tokenized_doc2:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc2.append(w)\n",
    "filtered_doc3 = [];\n",
    "for w in tokenized_doc3:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc3.append(w)\n",
    "filtered_doc4 = [];\n",
    "for w in tokenized_doc4:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc4.append(w)\n",
    "filtered_doc5 = [];\n",
    "for w in tokenized_doc5:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc5.append(w)\n",
    "filtered_doc6 = [];\n",
    "for w in tokenized_doc6:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_doc6.append(w)\n",
    "        \n",
    "print(\"Filterd document: \",filtered_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "\n",
    "puncts = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', \n",
    "          '{', '}', '[', ']', '|', ':', ';', '\"', \"'\", '<', '>', '?', '/', '\\\\', ',', '.']\n",
    "\n",
    "unpunct_doc1=[]\n",
    "for w in filtered_doc1:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc1.append(w)\n",
    "unpunct_doc2=[]\n",
    "for w in filtered_doc2:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc2.append(w)\n",
    "unpunct_doc3=[]\n",
    "for w in filtered_doc3:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc3.append(w)\n",
    "unpunct_doc4=[]\n",
    "for w in filtered_doc4:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc4.append(w)\n",
    "unpunct_doc5=[]\n",
    "for w in filtered_doc5:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc5.append(w)\n",
    "unpunct_doc6=[]\n",
    "for w in filtered_doc6:\n",
    "    if w not in puncts:\n",
    "        unpunct_doc6.append(w)\n",
    "\n",
    "print(\"No Punctuation document1: \",unpunct_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed Documents\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_doc1=[]\n",
    "for w in unpunct_doc1:\n",
    "    stemmed_doc1.append(ps.stem(w))\n",
    "stemmed_doc2=[]\n",
    "for w in unpunct_doc2:\n",
    "    stemmed_doc2.append(ps.stem(w))\n",
    "stemmed_doc3=[]\n",
    "for w in unpunct_doc3:\n",
    "    stemmed_doc3.append(ps.stem(w))\n",
    "stemmed_doc4=[]\n",
    "for w in unpunct_doc4:\n",
    "    stemmed_doc4.append(ps.stem(w))\n",
    "stemmed_doc5=[]\n",
    "for w in unpunct_doc5:\n",
    "    stemmed_doc5.append(ps.stem(w))\n",
    "stemmed_doc6=[]\n",
    "for w in unpunct_doc6:\n",
    "    stemmed_doc6.append(ps.stem(w))\n",
    "\n",
    "print(\"Stemmed document1: \", stemmed_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [stemmed_doc1, stemmed_doc2, stemmed_doc3, stemmed_doc4, stemmed_doc5, stemmed_doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute term frequecy counts using count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy1(doc):\n",
    "    return(doc)\n",
    "\n",
    "cvec = CountVectorizer(analyzer = 'word', tokenizer = dummy1, preprocessor=dummy1, token_pattern=None)\n",
    "X = cvec.fit_transform(corpus)\n",
    "\n",
    "print('Feature Names: ', '\\n', cvec.get_feature_names())\n",
    "print('\\n')\n",
    "print('Term - Document Matrix Dimensions:', '\\n', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term document matrix of frequency counts\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = cvec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "def dummy1(doc):\n",
    "    return(doc)\n",
    "\n",
    "tivec = TfidfVectorizer(analyzer = 'word', tokenizer = dummy1, preprocessor=dummy1, token_pattern=None)\n",
    "Y = tivec.fit_transform(corpus)\n",
    "\n",
    "print('Feature Names: ', '\\n', tivec.get_feature_names())\n",
    "print('\\n')\n",
    "print('Term - Document Matrix Dimensions:', '\\n', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y.toarray(), columns = tivec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################   End of Lab #############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
