{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of NLTK collection \n",
    "NLTK collection is a collection of sets of documents possibly from different sources. Each set is called a corpus. Some popular corpora in NLTK are\n",
    "\n",
    "* gutenberg\n",
    "* brown\n",
    "* word_net (thesaurus)\n",
    "* books\n",
    "* movie_reviews\n",
    "* chat_logs\n",
    "\n",
    "Since they are collected from different sources they may be stored in different formats like txt, xml etc. Therefore it is advisable to access them using only access tools provided by NLTK. \n",
    "\n",
    "On windows these documents are stores in the following structure \n",
    "\\users\\username\\AppData\\Roaming\\nltk_data\\corpora\n",
    "\n",
    "Each document is a text file. Following example shows how to access these file via NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "sent1 = sent_tokenize(sample)\n",
    "word1 = word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    print(sent1[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    print(word1[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The brown corpus:\n",
    "The brown corpus is a very popular corpus for research and model building in text mining. We will use this corpus to demonstate typical text mining tasks. These are\n",
    "* Listing files\n",
    "* Listing categories\n",
    "* Finding size of categories\n",
    "* Finding length of files\n",
    "* Studying lexical diversity\n",
    "* Tabulating word occurances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words(categories=['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "genre = ['hobbies', 'humor', 'science_fiction', 'news', 'romance', 'religion']\n",
    "\n",
    "for categ in genre:\n",
    "    tokens = len(brown.words(categories = [categ]))\n",
    "    types = len(set(brown.words(categories = [categ])))\n",
    "    ldiv = round(tokens/types, 2)\n",
    "    df1 = pd.DataFrame({'genre':categ, 'tokens':tokens, 'types': types, 'lex_diversity': ldiv}, index=[' '])\n",
    "    df = df.append(df1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (genre, word)\n",
    "      for genre in brown.categories()\n",
    "      for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of temporal analysis\n",
    "In this example we visualize how the words are being used at different times in history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "inaugural.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (target, fileid[:4])\n",
    "      for fileid in inaugural.fileids()\n",
    "      for w in inaugural.words(fileid)\n",
    "      for target in ['america', 'citizen']\n",
    "      if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting example of size of words used in different languages for expressing same idea. From the graph below you can see that english uses more small words than other european languages. (udhr = Universal Declaration of Human Rights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['English', 'German_Deutsch', 'Hungarian_Magyar']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (lang, len(word))\n",
    "      for lang in languages\n",
    "      for word in udhr.words(lang+ '-Latin1'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udhr.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['English-Latin1', 'German_Deutsch-Latin1', 'Hindi-UTF8']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (lang, len(word))\n",
    "      for lang in languages\n",
    "      for word in udhr.words(lang))\n",
    "cfd.plot(cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we guess the gender of a person from their names? See the illustartion below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids() \n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (fileid, name[-1])\n",
    "      for fileid in names.fileids()\n",
    "      for name in names.words(fileid))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tasks, we will demonstate how to get documents from different formats (or output from different software programs) like\n",
    "* Web (.htm, .html, .xml)\n",
    "* pdf (.pdf)\n",
    "* word (.docx)\n",
    "* text (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading raw text files\n",
    "\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text101 = nltk.Text(tokens)\n",
    "print(type(text101))\n",
    "print(text101[1020:1060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading HTML pages\n",
    "\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens[125:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[110:594]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read local files\n",
    "import os\n",
    "os.chdir(\"C:/Users/Sid/Desktop/Text Mining/Day1/Decks\")\n",
    "f = open('wolf.txt')\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('wolf.txt')\n",
    "for line in f:\n",
    "    print (line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading pdf files\n",
    "\n",
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFileObj = open('boy and wolf.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "print(pdfReader.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageObj = pdfReader.getPage(0)\n",
    "f = pageObj.extractText()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "doc = docx.Document('boy and wolf.docx')\n",
    "len(doc.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.paragraphs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.paragraphs[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  End of Lab ###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
