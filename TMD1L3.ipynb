{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Term Matrix\n",
    "In this lab we will learn about all steps in the construction of term-document matrix. These steps are\n",
    "* Tokenize text\n",
    "* Remove punctutions\n",
    "* Remove stopwords\n",
    "* Stem the words to root form\n",
    "* Synonymy\n",
    "* POS tagging\n",
    "\n",
    "For this we build a sample dcument on Indian citizenship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sententence tokenization. Breaks text into sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"India is my country! All indians are my brothers and (sisters); I love my country and am proud to be an Indian. \n",
    "Our nationality is Indian national. India has rich culture: Tiger is the National animal and peacock is national bird. \n",
    "Our nation is great. India@cross-roads / India is socialist nation. We nationalized 14 banks + railways. Indian society \n",
    "is very sociable. Every indian should discharge social tasks & duties. Nationalism is a halmark of india culture. India is culturally diverse. \n",
    "We are part of United Nations. Should we nationalize all natural assests? You should do your duty to your country.\"\"\"\n",
    "\n",
    "tokenized_sent=sent_tokenize(text)\n",
    "print(tokenized_sent)\n",
    "print(len(tokenized_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization. Break text into tokens (i.e. words are terms)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_text=word_tokenize(text)\n",
    "print(tokenized_text)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining frequency distributions using FreqDist() function\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_text)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are 5 most commonly used words?\n",
    "\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "\n",
    "fdist.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "\n",
    "filtered_text=[]\n",
    "for w in tokenized_text:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "\n",
    "print(\"Tokenized text:\",tokenized_text)\n",
    "print('\\n')\n",
    "print(\"Filterd text:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "\n",
    "puncts = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', \n",
    "          '{', '}', '[', ']', '|', ':', ';', '\"', \"'\", '<', '>', '?', '/', '\\\\', ',', '.']\n",
    "\n",
    "nopunct_text=[]\n",
    "for w in filtered_text:\n",
    "    if w not in puncts:\n",
    "        nopunct_text.append(w)\n",
    "\n",
    "print(\"Tokenized text:\",tokenized_text)\n",
    "print('\\n')\n",
    "print(\"No Punctuation text:\",nopunct_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_text=[]\n",
    "for w in nopunct_text:\n",
    "    stemmed_text.append(ps.stem(w))\n",
    "\n",
    "print(\"No punctuation Text: \", nopunct_text)\n",
    "print('\\n')\n",
    "print(\"Stemmed Text: \", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = wordnet.synsets(\"program\")\n",
    "print(syn[0].name())\n",
    "print(syn[0].lemmas()[0].name())\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print('\\n')\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################### End of Lab ######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
